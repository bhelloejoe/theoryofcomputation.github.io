<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Page title</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha512-MV7K8+y+gLIBoVD59lQIYicR65iaqukzvf/nwasF0nqhPay5w/9lJmVM2hMDcnK1OnMGCdVK+iQrJ7lzPJQd1w==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <div class="navbar">
            <div class="logo"><a href="#logo">E-Learning</a></div>
            <ul class="link">
                <li><a href="#home">Home</a></li>
                <li><a href="#topic">Topic</a></li>
                <li><a href="#member">Members</a></li>
            </ul>
            <a href="#start" class="start">Get Started</a>
            <div class="toggle">
                 <i class="fa-solid fa-bars"></i>
            </div>
            <div class="menu">
					<li><a href="#home">Home</a></li>
                <li><a href="#topic">Topic</a></li>
                <li><a href="#member">Members</a></li>
                <li><a href="#start" class="starts">Get Started</a></li>
            </div>
        </div>
    </header>
	<script>
        const toggleBtn = document.querySelector('.toggle')
        const toggleBtnIcon = document.querySelector('.toggle i')
        const menu = document.querySelector('.menu')
        
        toggleBtn.onclick = function() {
            menu.classList.toggle('open')
            const isOpen = menu.classList.contains('open')
            
            toggleBtnIcon.classList =isOpen
            ?'fa-solid fa-xmark'
            :'fa-solid fa-bars'
        }
    </script>
	<div class="container">
		<section id="home">
            <div class="title">
                <small>Group 5</small>
                <h1 data-text="Theory Of Computation">Theory Of Computation</h1>
            </div>
        </section>
		<section class="txtstart" id="start">
			<div class="startinfo">
			<div class="tocmeaning">
				<h2>Theory Of Computation(TOC)</h2></br></br>
				<p>is a branch of Computer Science that is concerned with how problems can be solved using algorithms and how efficiently they can be solved.</br></br>
				Real-world computers perform computations that by nature run like mathematical models to solve problems in systematic ways. The essence of the theory of computation is to help develop mathematical and logical models that run efficiently and to the point of halting. Since all machines that implement logic apply TOC, studying TOC gives learners an insight into computer hardware and software limitations.</p></br></br>
				<h2>Importance of Theory of computation</h2></br></br>
				<p><b>The theory of computation forms the basis for:</b></p></br></br>
				<ul>
					<li><p>Writing efficient algorithms that run in computing devices.</p></li></br>
					<li><p>Programming language research and their development.</p></li></br>
					<li><p>Efficient compiler design and construction.</p></li>
				</ul>
			</div>
				<img src="img5.jpg">
			</div>
			<div class="tocinfo">
				<h2>Key considerations of computational problems</h2></br></br>
				<ul>
					<li><p>What can and cannot be computed.</p></li></br>
					<li><p>Speed of such computations.</p></li></br>
					<li><p>The amount of memory in use during such computations.</p></li></br></br>
				</ul></br></br>
				<h2>Advantages of Theory of Computation</h2></br></br>
				<ol>
					<li><p>Theory of Computation deals with how efficiently any algorithm would solve any computational problem. Also, abstract machines are introduced in the Computational theory, which are defined mathematically. Hence, the algorithms would not need to change every time any physical hardware gets enhanced.</p></li></br>
					<li><p>There is a massive amount of work that has been made possible in the portion of NLP (Natural Language Processing) that involves the construction of FSMs (Finite State Machines), also known as FSA (Finite State Automata).</p></li></br>
					<li><p>Theory of Computation has helped in many fields such as Cryptography, Design and Analysis of Algorithm, Quantum Calculation, Logic within Computer Science, Computational Difficulty, Randomness within Calculation and Correcting Errors in Codes</p></li></br>
					<li><p>A computational model can cope with complexity in ways that verbal arguments cannot, resulting in satisfactory answers for what would otherwise be ambiguous hand-wavy arguments. Furthermore, computational models can manage complexity at several levels of analysis, allowing data from various levels to be integrated and connected.</p></li></br></br>
				</ol>
				<h2>History</h2></br></br>
				<p>
					The theory of computation can be considered the creation of models of all kinds in the field of computer science. Therefore, mathematics and logic are used. In the last century it became an independent academic discipline and was separated from mathematics.</br>
					Some pioneers of the theory of computation were Ramon Llull, Alonzo Church, Kurt Gödel, Alan Turing, Stephen Kleene, Rózsa Péter, John von Neumann and Claude Shannon.
				</p></br></br>
			</div>
		</section>
		<section id="topic">
			<div class="topic-link" id="automata-link">
				<div class="glass-effect">
				</div>
				<div class="topic-title">
					<small>Theory Of Computation</small>
					<h2>Automata Theory</h2>
					<p>a theoretical branch of computer science and mathematical. It is the study of abstract machines and the computation problems that can be solved using these machines. The abstract machine is called the automata. The main motivation behind developing the automata theory was to develop methods to describe and analyse the dynamic behaviour of discrete systems.</p></br>
					<a href="#automata" class="btn">Learn Automata Theory</a>
				</div>
				<div class="topic-list">
					<a  href="#automata-link">
						<div class="thumbnail" style="background:url('img3.jpg')">
							<div class="details">
								<p class="TOC">Theory Of Computation</p>
								<h2>Automata Theory</h2>
							</div>	
						</div>
					</a>
					<a  href="#complexity-link">
						<div class="thumbnail" style="background:url('img9.jpg')">
							<div class="details">
								<p class="TOC">Theory Of Computation</p>
								<h2>Complexity Theory</h2>
							</div>
						</div>
					</a>
					<a  href="#computability-link">
						<div class="thumbnail" style="background:url('img8.jpg')">
							<div class="details">
								<p class="TOC">Theory Of Computation</p>
								<h2>Computability Theory</h2>
							</div>
						</div>
					</a>
				</div>
			</div>
			<div class="topic-link2" id="complexity-link">
				<div class="glass-effect">
				</div>
				<div class="topic-title">
					<small>Theory Of Computation</small>
					<h2>Complexity Theory</h2>
					<p>This theoretical computer science branch is all about studying the cost of solving problems while focusing on resources (time & space) needed as the metric. The running time of an algorithm varies with the inputs and usually grows with the size of the inputs.</p></br>
					<a href="#complexity" class="btn">Learn Compexity Theory</a>
				</div>
				<div class="topic-list">
					<a  href="#automata-link">
						<div class="thumbnail" style="background:url('img3.jpg')">
							<div class="details">
								<p class="TOC">Theory Of Computation</p>
								<h2>Automata Theory</h2>
							</div>	
						</div>
					</a>
					<a  href="#complexity-link">
						<div class="thumbnail" style="background:url('img9.jpg')">
							<div class="details">
								<p class="TOC">Theory Of Computation</p>
								<h2>Complexity Theory</h2>
							</div>
						</div>
					</a>
					<a  href="#computability-link">
						<div class="thumbnail" style="background:url('img8.jpg')">
							<div class="details">
								<p class="TOC">Theory Of Computation</p>
								<h2>Computability Theory</h2>
							</div>
						</div>
					</a>
				</div>
			</div>
			<div class="topic-link3" id="computability-link">
				<div class="glass-effect">
				</div>
				<div class="topic-title">
					<small>Theory Of Computation</small>
					<h2>Computability Theory</h2>
					<p>Defines whether a problem is “solvable” by any abstract machine. Some problems are computable while others are not. Computation is done by various computation models depending on the nature of the problem at hand, examples of these machines are: the Turing machine, Finite state machines, and many others.</p></br>
					<a href="#computability" class="btn">Learn Computability Theory</a>
				</div>
				<div class="topic-list">
					<a  href="#automata-link">
						<div class="thumbnail" style="background:url('img3.jpg')">
							<div class="details">
								<p class="TOC">Theory Of Computation</p>
								<h2>Automata Theory</h2>
							</div>	
						</div>
					</a>
					<a  href="#complexity-link">
						<div class="thumbnail" style="background:url('img9.jpg')">
							<div class="details">
								<p class="TOC">Theory Of Computation</p>
								<h2>Complexity Theory</h2>
							</div>
						</div>
					</a>
					<a  href="#ocmputability-link">
						<div class="thumbnail" style="background:url('img8.jpg')">
							<div class="details">
								<p class="TOC">Theory Of Computation</p>
								<h2>Computability Theory</h2>
							</div>
						</div>
					</a>
				</div>
			</div>
		</section>
		<section id="member">
			<div class="mem">
				<center><img src="jet.jpg"></center></br>
				<div class="info">
					<h2>Bhelloejoe B. Dumagan</h2></br></br>
					<p><b>Birthdate:</b> December 5,2002</p></br>
					<p><b>Age:</b> 20</p></br>
					<p><b>Address:</b> Purok 2 Doyos, Carrascal Surigao del Sur</p>
				</div>
				
			</div></br></br>
			<div class="mem">
				<center><img src="jessa.jpg"></center></br>
				<div class="info">
					<h2>Jessa Mae P. Celeste</h2></br></br>
					<p><b>Birthdate:</b> June 20,2000</p></br>
					<p><b>Age:</b> 22</p></br>
					<p><b>Address:</b> Baybay Carrascal Surigao del Sur</p>
				</div>
			</div></br></br>
			<div class="mem">
				<center><img src="cherry.jpg"></center></br>
				<div class="info">
					<h2>Cherry Joy E. Solana</h2></br></br>
					<p><b>Birthdate:</b> November 20, 2001</p></br>
					<p><b>Age:</b> 21</p></br>
					<p><b>Address:</b> Hayanggabon, Claver, Surigao del Sur</p>
				</div>
				
			</div></br></br>
			<div class="mem">
				<center><img src="rona.jpg"></center></br>
				<div class="info">
					<h2>Ronalyn L. Peruda</h2></br></br>
					<p><b>Birthdate:</b> October 28,2002</p></br>
					<p><b>Age:</b> 20</p></br>
					<p><b>Address:</b> Ipil, Gigaquit, Surigao del Sur</p>
				</div>
			</div></br></br>
			<div class="mem">
				<center><img src="jessaly.jpg"></center></br>
				<div class="info">
					<h2>Jessaly Q. Rosil</h2></br></br>
					<p><b>Birthdate:</b> June 26, 2001</p></br>
					<p><b>Age:</b> 21</p></br>
					<p><b>Address:</b> Purok 4 Pag-Antayan, Cantilan Surigao del Sur</p>
				</div>
				
			</div></br></br>
		</section></br></br></br>
		<section id="automata">
		<div class="background">
		<div class="glass-effect">
		</div>
		<div class="topic-title">
			<small>Theory Of Computation</small>
			<h2>Automata Theory</h2>
			<p>a theoretical branch of computer science and mathematical. It is the study of abstract machines and the computation problems that can be solved using these machines. The abstract machine is called the automata. The main motivation behind developing the automata theory was to develop methods to describe and analyse the dynamic behaviour of discrete systems.</p></br>
		</div>
	</div>
		<div class="txtss">
			<p><b>Branches of Automata theory</b></p></br></br>
				<ul>
					<li><p>Finite Automata (FA): This is a computer model that is inferior in its computation ability. This model is fit for devices with limited memory. It is a simple abstract machine with five elements that define its functioning and processing of problems.</p></li></br></br>
				</ul>
				<p>A <b>Finite Automaton (FA)</b> is a finite collection of states with rules (transition functions) for traversing through the states depending on the input symbol. FA accepts or rejects input strings while reading the strings from left to right.</p></br></br>
				<p><b>The tuples are:</b></p></br></br>
				<ul>
					<li><p><b>Q:</b> Finite set of states.</p></li></br></br>
					<li><p><b>∑:</b> Set of input symbols.</li></br></br>
					<li><p><b>q:</b> Initial state.</p></li></br></br>
					<li><p><b>F:</b> Set of final states.</p></li></br></br>
					<li><p><b>δ:</b> Transition function.</p></li></br></br>
				</ul>
				<p>Finite Automata is useful in building text editors/text preprocessors. FA are poor models of computers. They can only perform simple computational tasks.</p></br></br>
				<ul>
					<li><p><b>Context-Free Grammars (CFGs):</b> They are more powerful abstract models than FA and are essentially used in the programming languages and natural language research work.</p></li></br></br>
					<li><p><b>Turing Machines:</b> They are abstract models for real computers having an infinite memory (in the form of a tape) and a reading head. They form much more powerful computation models than FA, CFGs, and Regular Expressions.</p></li></br></br>
				</ul>
				<p>This automation consists of states and transitions. The State is represented by circles, and the Transitions is represented by arrows.</br></br>
				Automata is the kind of machine which takes some string as input and this input goes through a finite number of states and may state.</p></br></br>
				<p><b>There are the basic terminologies that are important and frequently used in automata:</b></p></br></br>
				<p><b>Symbols:</b></p></br></br>
				<p>Symbols are an entity or individual objects, which can be any letter, alphabet or any picture.</p></br></br>
				<p><b>Example:</b></p></br></br>
				<p>1, a, b, #</p></br></br>
				<p><b>Alphabets:</b></p></br></br>
				<p>Alphabets are a finite set of symbols. It is denoted by ∑.</p></br></br>
				<p><b>Examples:</b></p></br></br>
				<div class="example">
					<p>∑ = {a, b}</br></br>
						∑ = {A, B, C, D}</br></br>
						∑ = {0, 1, 2}</br></br>
						∑ = {0, 1, ....., 5]</br></br>
						∑ = {#, β, Δ}</p>
				</div></br></br>
				<p><b>String:</b><p></br></br>
				<p>It is a finite collection of symbols from the alphabet. The string is denoted by w.</p></br></br>
				<p><b>Example 1:</b></p></br></br>
				<p>If ∑ = {a, b}, various string that can be generated from ∑ are {ab, aa, aaa, bb, bbb, ba, aba.....}.</p></br></br>
				<ul>
					<li><p>A string with zero occurrences of symbols is known as an empty string. It is represented by ε.</p></li></br></br>
					<li><p>The number of symbols in a string w is called the length of a string. It is denoted by |w|</p></li></br></br>
				</ul>
				<p><b>Example 2:</b></p></br></br>
				<div class="example">
					<p>w = 010</br></br>
					Number of Sting |w| = 3</p>
				</div></br>
				<p><b>Language:</b></p></br></br>
				<p>A language is a collection of appropriate string. A language which is formed over Σ can be Finite or Infinite</p></br></br>
				<p><b>Example: 1</b></p></br></br>
				<div class="example">
					<p>L1 = {Set of string of length 2}</p></br></br>
					<p>= {a, aa, aaa, abb, abbb, ababb} &nbsp; &nbsp; &nbsp; <b>Infinite Language</b></p>
				</div></br>
				<p><b>Example: 2</b></p></br></br>
				<div class="example">
					<p>L2 = {Set of all strings starts with 'a'}</p></br></br>
					<p>= {a, aa, aaa, abb, abbb, ababb} &nbsp; &nbsp; &nbsp; <b>Infinite Language</b></p>
				</div></br></br>
				<ul>
					<li><p>Finite automata are used to recognize patterns.</p></li></br></br>
					<li><p>It takes the string of symbol as input and changes its state accordingly. When the desired symbol is found, then the transition occurs.</p></li></br></br>
					<li><p>At the time of transition, the automata can either move to the next state or stay in the same state.</p></li></br></br>
					<li><p>Finite automata have two states, <b>Accept state</b> or <b>Reject state</b>. When the input string is processed successfully, and the automata reached its final state, then it will accept.</p></li></br></br>
				</ul>
				<p><b>Formal Definition of FA</b></p></br></br>
				<p>A finite automaton is a collection of 5-tuple (Q, ∑, δ, q0, F), where:</p></br></br>
				<div class="example">
					<p>
						Q: finite set of states</br></br>
						∑: finite set of the input symbol</br></br>
						q0: initial state</br></br>
						F: final state</br></br>
						δ: Transition function
					</p>
				</div></br></br>
				<h1><b>Finite Automata Model:</b></h1></br></br>
				<p>
					Finite automata can be represented by input tape and finite control.</br></br>
					<b>Input tape:</b> It is a linear tape having some number of cells. Each input symbol is placed in each cell.</br></br>
					<b>Finite control:</b> The finite control decides the next state on receiving particular input from input tape. The tape reader reads the cells one by one from left to right, and at a time only one input symbol is read.</br></br>
				</p>
				<center><img src="img9.png"></center></br></br>
				<h1><b>Types of Automata:</b></h1></br></br>
				<p>
					There are two types of finite automata:</br></br>
					<ol>
						<li><p>DFA(deterministic finite automata)</p></li></br></br>
						<li><p>NFA(non-deterministic finite automata)</p></li></br></br>
					</ol>
				</p>
				<center><img src="img10.png" width="600px"></center></br></br>
				<h1><b>1.DFA</b></h1></br>
				<p>DFA refers to deterministic finite automata. Deterministic refers to the uniqueness of the computation. In the DFA, the machine goes to one state only for a particular input character. DFA does not accept the null move.</p></br></br>
				<h1><b>2. NFA</b></h1></br>
				<p>NFA stands for non-deterministic finite automata. It is used to transmit any number of states for a particular input. It can accept the null move.</p></br></br>
				<h1><b>Some important points about DFA and NFA:</b></h1></br></br>
				<ol>
					<li><p>Every DFA is NFA, but NFA is not DFA.</p></li></br></br>
					<li><p>There can be multiple final states in both NFA and DFA.</p></li></br></br>
					<li><p>DFA is used in Lexical Analysis in Compiler.</p></li></br></br>
					<li><p>NFA is more of a theoretical concept.</p></li></br></br>
				</ol>
			</div>
		</section>
		<section id="complexity">
			<div class="background">
				<div class="glass-effect">
				</div>
				<div class="topic-title">
					<small>Theory Of Computation</small>
					<h2>Complexity Theory</h2>
					<p>This theoretical computer science branch is all about studying the cost of solving problems while focusing on resources (time & space) needed as the metric. The running time of an algorithm varies with the inputs and usually grows with the size of the inputs.</p></br>
				</div>
			</div>
			<div class="txtss">
				<center><h2><b>Measuring Complexity</b></h2></center></br></br>
			<p>Measuring complexity involves an algorithm analysis to determine how much time it takes while solving a problem (time complexity). To evaluate an algorithm, a focus is made on relative rates of growth as the size of the input grows.</p></br></br>
			<p>Since the exact running time of an algorithm often is a complex expression, we usually just estimate it. We measure an algorithm’s time requirement as a function of the input size (n) when determining the time complexity of an algorithm.</p></br></br>
			<p>As T(n), the time complexity is expressed using the Big O notation where only the highest order term in the algebraic expressions are considered while ignoring constant values.</p></br></br>
			<p><b>The common running times when analyzing algorithms are:</b></p></br></br>
			<ul>
				<li><p><b>O(1)</b> - Constant time or constant space regardless of the input size.</li></p></br></br>
				<li><p><b>O(n)</b> - Linear time or linear space, where the requirement increases uniformly with the size of the input.</li></p></br></br>
				<li><p><b>O(log n)</b> - Logarithmic time, where the requirement increases in a logarthimic nature.</li></p></br></br>
				<li><p><b>O(n^2)</b> - Quadratic time, where the requirement increases in a quadratic nature.</li></p></br></br>	
			</ul>
			<p>This analysis is based on 2 bounds that can be used to define the cost of each algorithm.</p></br></br>
			<p><b>They are:</b></p></br></br>
			<ul>
				<li><p>Upper (Worst Case Scenario)</p></li></br></br>
				<li><p>Lower (Best Case Scenario)</p></li></br></br>
			</ul>
			<p><b>The major classifications of complexities include:</b></p></br></br>
			<ul>
				<li><p>Class P: The class P consists of those problems that are solvable in polynomial time. These are problems that can be solved in time O(n^k) for some constant k where n is the input size to the problem. It is devised to capture the notion of efficient computation.</p></li></br></br>
				<li><p>Class NP: It forms the class of all problems whose solution can be achieved in polynomial time by non-deterministic Turing machine. NP is a complexity class used to classify decision problems.</p></li></br></br>
			</ul>
			<p>A major contributor to the complexity theory is the complexity of the algorithm used to solve the problem. Among several algorithms used in solving computational problems are those whose complexity can range from fairly complex to very complex.</p></br></br>
			<p>The more complex an algorithm, the more computational complexity will be in a given problem.</p></br></br>
			<p><b>Factors that influence program efficiency</b></p></br></br>
			<ul>
				<li><p>The problem being solved.</p></li></br></br>
				<li><p>The algorithm used to build the program.</p></li></br></br>
				<li><p>Computer hardware.</p></li></br></br>
				<li><p>Programming language used.</p></li></br></br>
			</ul>
			<p><b>Conclusion</b></p></br></br>
			<p>Programs are formally written from descriptions of computations for execution on machines. We’ve learned that TOC is concerned with a formalism that helps build efficient programs. Efficient algorithms lead to better programs that optimally use hardware resources.</p></br></br>
			<p>Good understanding of the Theory of Computation helps programmers and developers express themselves clearly and intuitively, thus avoiding entering into potentially uncomputable problems while working with computational models.</p></br></br>
			<h2><b>1. On computational complexity</b></h2></br></br>
			<p>
				Central to the development of computational complexity theory is the notion of a decision problem. Such a problem corresponds to a set X in which we wish to decide membership. For instance the problem PRIMES corresponds to the subset of the natural numbers which are prime – i.e. {n∈N∣n is prime}. Decision problems are typically specified in the form of questions about a class of mathematical objects whose positive instances determine the set in question – e.g.</br></br>
				SAT  Given a formula ϕ of propositional logic, does there exist a satisfying assignment for ϕ?</br></br>
				TRAVELING SALESMAN (TSP)  Given a list of cities V, the integer distance d(u,v) between each pair of cities u,v∈V, and a budget b∈N, is there a tour visiting each city exactly once and returning to the starting city of total distance ≤b?</br></br>
				INTEGER PROGRAMMING   Given an n×m integer matrix A and an n-dimensional vector of integers b⃗ , does there exist an m-dimensional vector x⃗  of integers such that Ax⃗ =b?</br></br>
				PERFECT MATCHING   Given a finite bipartite graph G, does there exist a perfect matching in G? (G is bipartite just in case its vertices can be partitioned into two disjoints sets U and V such that all of its edges E connect a vertex in U to one in V. A matching is a subset of edges M⊆E no two members of which share a common vertex. M is perfect if it matches all vertices.)</br></br></br>
				These problems are typical of those studied in complexity theory in two fundamental respects. First, they are all effectively decidable. This is to say that they may all be decided in the ‘in principle’ sense studied in computability theory – i.e. by an effective procedure which halts in finitely many steps for all inputs. Second, they arise in contexts in which we are interested in solving not only isolated instances of the problem in question, but rather in developing methods which allow it to be efficiently solved on a mass scale – i.e. for all instances in which we might be practically concerned. Such interest often arises in virtue of the relationship of computational problems to practical tasks which we seek to analyze using the methods of discrete mathematics. For example, instances of SAT arise when we wish to check the consistency of a set of specifications (e.g. those which might arise in scheduling the sessions of a conference or designing a circuit board), instances of TSP and INTEGER PROGRAMMING arise in many logistical and planning applications, instances of PERFECT MATCHING arise when we wish to find an optimal means of pairing candidates with jobs, etc.[1]</br></br></br>
				The resources involved in carrying out an algorithm to decide an instance of a problems can typically be measured in terms of the number of processor cycles (i.e. elementary computational steps) and the amount of memory space (i.e. storage for auxiliary calculations) which are required to return a solution. The methods of complexity theory can be useful not only in deciding how we can most efficiently expend such resources, but also in helping us to distinguish which effectively decidable problems possess efficient decision methods in the first place. In this regard, it is traditional to distinguish pre-theoretically between the class of feasibly decidable problems – i.e. those which can be solved in practice by an efficient algorithm – and the class of intractable problems – i.e. those which lack such algorithms and may thus be regarded as intrinsically difficult to decide (despite possibly being decidable in principle). The significance of this distinction is most readily appreciated by considering some additional examples.</br></br>
				<h1><b>1.1 A preliminary example</b></h1></br></br>
				A familiar example of a computational problem is that of primality testing – i.e. that of deciding n∈PRIMES? This problem was intensely studied in mathematics long before the development of digital computers. (See, e.g., (Williams 1998) for a history of primality testing and (Crandall and Pomerance 2005) for a recent survey of the state of the art.) After a number of preliminary results in the 19th and 20th centuries, the problem PRIMES was shown in 2004 to possess a so-called polynomial time decision algorithm – i.e. the so-called AKS primality test (Agrawal, Kayal, and Saxena 2004). This qualifies PRIMES as feasibly decidable relative to the standards which are now widely accepted in complexity theory and algorithmic analysis</br></br></br>
				Two related problems can be used to illustrate the sort of contrasts in difficulty which complexity theorists seek to analyze:</br></br>
				RELATIVE PRIMALITY  Given natural numbers x and y, do x and y possess greatest common divisor 1? (I.e. are x and y relatively prime?)</br></br>
				FACTORIZATION  Given natural numbers x and y, does there exist 1<d≤y such that d∣x?</br></br></br>
				RELATIVE PRIMALITY can be solved by applying Euclid’s greatest common divisor algorithm – i.e. on input y≤x, repeatedly compute the remainders r0=rem(x,y), r1=rem(y,r0), r2=rem(r0,r1) …, until ri=0 and then return ‘yes’ if ri−1=1 and ‘no’ otherwise. It may be shown that the number of steps in this sequence is always less than or equal to 5⋅log10(x).[2] This means that in order to determine if x and y are relatively prime, it suffices to calculate a number of remainders which is proportional to the number of digits in the decimal representation of the smaller of the two numbers. As this may also be accomplished by an efficient algorithm (e.g. long division), it may plausibly be maintained that if we are capable of inscribing a pair of numbers x,y – e.g. by writing their numerical representations in binary or decimal notation on a blackboard or by storing such numerals in the memory of a digital computer of current design – then either we or such a computer will also be able to carry out these algorithms in order to decide whether x and y are relatively prime. This is the hallmark of a feasibly decidable problem – i.e. one which can be decided in the ‘in practice’ sense of everyday concretely embodied computation.</br></br></br>
				FACTORIZATION is a decision variant of the familiar problem of finding the prime factorization of a given number x – i.e. the unique sequence of primes pi and exponents ai such that x=pa11⋅…⋅pakk. It is not difficult to see that if there existed an efficient algorithm for deciding FACTORIZATION, then there would also exist an efficient algorithm for determining prime factorizations.[3] It is also easy to see that the function taking x to its prime factorization is effectively computable in the traditional sense of computability theory. For instance, it can be computed by the trial division algorithm.</br></br></br>
				In its simplest form, trial division operates by successively testing x for divisibility by each integer smaller than x and keeping track of the divisors which have been found thus far. As the number of divisions required by this procedure is proportional to x itself, it might at first seem that it is not a particularly onerous task to employ this method to factor numbers of moderate size using paper and pencil calculation – say x<100000. Note, however, that we conventionally denote natural numbers using positional notations such as binary or decimal numerals. A consequence of this is that the length of the expression which is typically supplied as an input to a numerical algorithm to represent an input x∈N is proportional not to x itself, but rather to logb(x) where b≥2 is the base of the notation system in question.[4] As a consequence it is possible to concretely inscribe positional numerals of moderate length which denote astronomically large numbers. For instance a binary numeral of 60 digits denotes a number which is larger than the estimated age of the universe in seconds and a binary numeral of 250 digits denotes a number which is larger than the estimated age of the universe in Planck times.[5]</br></br></br>
				There are thus natural numbers whose binary representations we can easily inscribe, but for which no human mathematician or foreseeable computing device can carry out the trial division algorithm. This again might not seem particularly troubling as this algorithm is indeed ‘naive’ in the sense that it admits to several obvious improvements – e.g. we need only test x for divisibility by the numbers 2,…,x−−√ to find an initial factor, and of these we need only test those which are themselves prime (finitely many of which can be stored in a lookup table). Nonetheless, mathematicians have been attempting to find more efficient methods of factorization for several hundred years. The most efficient factorization algorithm yet developed is similar to the trial division algorithm in that it requires a number of primitive steps which grows roughly in proportion to x (i.e. the size of its input, as opposed to the length of its binary representation).[6] A consequence of these observations is that there exist concretely inscribable numbers – say on the order of 400 decimal digits – with the following properties: (i) we are currently unaware of their factorizations; and (ii) it is highly unlikely we could currently find them even if we had access to whatever combination of currently available computing equipment and algorithms we wish.</br></br></br>
				Like the problems introduced above, FACTORIZATION is of considerable practical importance, perhaps most famously because the security of well known cryptographic protocols assume that it is intractable in the general case (see, e.g., Cormen, Leiserson, and Rivest 2005). But the foregoing observations still do not entail any fundamental limitation on our ability to know a number’s prime factorization. For it might still be hoped that further research will yield a more efficient algorithm which will allow us to determine the prime factorization of every number x in which we might take a practical interest. A comparison of Euclid’s algorithm and trial division again provides a useful context for describing the properties which we might expect such an algorithm to possess. For note that the prior observations suggest that we ought to measure the size of the input x∈N to a numerical algorithm not by x itself, but rather in terms of the length of x’s binary representation. If we let |x|=dflog2(x) denote this quantity, then it is easy to see that the efficiency of Euclid’s algorithm is given by a function which grows proportionally to |x|c1 for fixed c1 (in fact, c1=1), whereas the efficiency of trial division is given by a function proportional c|x|2 for fixed c2 (in fact, c2=2).</br></br></br>
				The difference in the growth rate of these functions illustrates the contrast between polynomial time complexity – which is currently taken by complexity theorists as the touchstone of feasibility – and exponential time complexity – which has traditionally been taken as the touchstone of intractability. For instance, if it could be shown that no polynomial time factorization algorithm exists, it might then seem reasonable to conclude that FACTORIZATION is a genuinely intractable problem.</br></br></br>
				Although it is currently unknown whether this is the case, contemporary results provide circumstantial evidence that FACTORIZATION is indeed intractable (see Section 3.4.1). Stronger evidence can be adduced for the intractability of conjecture SAT, TSP, and INTEGER PROGRAMMING (and similarly for a great many other problems of practical interest in subjects like logic, graph theory, linear algebra, formal language theory, game theory, and combinatorics). The technical development of complexity theory aims to make such comparisons of computational difficulty precise and to show that the classification of certain problems as intractable admits to rigorous mathematical analysis.</br></br></br></br>
				<h1><b>1.2 Basic conventions</b></h1></br></br>
				As we have just seen, in computational complexity theory a problem X is considered to be complex in proportion to the difficulty of carrying out the most efficient algorithm by which it may be decided. Similarly, one problem X is understood to be more complex (or harder) than another problem Y just in case Y possesses a more efficient decision algorithm than the most efficient algorithm for deciding X. In order to make these definitions precise, a number of technical conventions are employed, many of which are borrowed from the adjacent fields of computability theory (e.g. Rogers 1987) and algorithmic analysis (e.g. Cormen, Leiserson, and Rivest 2005). It will be useful to summarize these before proceeding further.</br></br>
				<ul>
					<li><p>A reference model of computation M is chosen to represent algorithms. M is assumed to be a reasonable model in the sense that it accurately reflects the computational costs of carrying out the sorts of informally specified algorithms which are encountered in mathematical practice. The deterministic Turing machine model T is traditionally selected for this purpose. (See Section 2.2 for further discussion of reasonable models and the justification of this choice.)</p></li></br></br></br>
					<li><p>Decision problems are represented as sets consisting of objects which can serve as the inputs for a machine M∈M. For instance, if T is used as the reference model then it is assumed that all problems X are represented as sets of finite binary strings – i.e. X⊆{0,1}∗. This is accomplished by defining a mapping ┌⋅┐:X→{0,1}∗ whose definition will depend on the type of objects which comprise X. For instance, if X⊆N, then ┌n┐ will typically be the binary numeral representing n. And if X is a subset of FormL – i.e. the set of formulas over a formal language L such as that of propositional logic – then ┌ϕ┐ will typically be a (binary) Gödel numeral for ϕ. Based on these conventions, problems X will henceforth be identified with sets of strings {┌x┐:x∈X}⊆{0,1}∗ (which are often referred to as languages) corresponding to their images under such an encoding.</p></li></br></br></br>
					<li><p>A machine M is said to decide a language X just in case M computes the characteristic function of X relative to the standard input-output conventions for the model M. For instance, a Turing machine T decides X just in case for all x∈{0,1}∗, the result of applying T to x yields a halting computation ending in a designated accept state if x∈X and a designated reject state if x∉X. A function problem is that of computing the values of a given function f:A→B. M is said to solve a function problem f:A→B just in case the mapping induced by its operation coincides with f(x) – i.e. if M(x)=f(x) for all x∈A where M(x) denotes the result of applying machine M to input x, again relative to the input-output conventions for the model M.</li></p></br></br></br>
					<li><p>For each problem X, it is also assumed that an appropriate notion of problem size is defined for its instances. Formally, this is a function |⋅|:X→N chosen so that the efficiency of a decision algorithm for X will varying uniformly in |x|. As we have seen, if X⊆N, it is standard to take |n|=log2(n) – i.e. the number of digits (or length) of the binary numeral ┌n┐ representing n. Similarly if X is a class of logical formulas over a language L (e.g. that or propositional or first-order logic), then |ϕ| will typically be a measure of ϕ’s syntactic complexity (e.g. the number of propositional variables or clauses it contains). If X is a graph theoretic problem its instances will consist of the encodings of finite graphs of the form G=⟨V,E⟩ where V is a set of vertices and E⊆V×V is a set of edges. In this case |G| will typically be a function of the cardinalities of the sets V and E.</p></li></br></br></br>
					<li><p>The efficiency of a machine M is measured in terms of its time complexity – i.e. the number of basic steps timeM(x) required for M to halt and return an output for the input x (where the precise notion of ‘basic step’ will vary with the model M). This measure may be converted into a function of type N→N by considering tM(n)=max{timeM(x):|x|=n} – i.e. the worst case time complexity of M defined as the maximum number of basic steps required for M to halt and return an output for all inputs x of size n. The worst case space complexity of M – denoted sM(n) – is defined similarly – i.e. the maximum number of tape cells (or other form of memory locations) visited or written to in the course of M’s computation for all inputs of size n.</p></li></br></br></br>
					<li><p>The efficiency of two machines is compared according to the order of growth of their time and space complexities. In particular, given a function f:N→N we define its order of growth to be O(f(n))={g(n):∃c∃n0∀n≥n0(g(n)<c⋅f(n))} – i.e. the set of all functions which are asymptotically bounded by f(n), ignoring scalar factors. For instance, for all fixed k,c1,c2 ∈N the following functions are all in O(n2): the constant k-function, log(n),n,c1⋅n2+c2. However 11000n3∉O(n2). A machine M1 is said to have lower time complexity (or to run faster than) another machine M2 if tM1(n)∈O(tM2(n)), but not conversely. Space complexity comparisons between machines are performed similarly.</p></li></br></br></br>
					<li><p>The time and space complexity of a problem X are measured in terms of the worst case time and space complexity of the asymptotically most efficient algorithm for deciding X. In particular, we say that X has time complexity O(t(n)) if the worst case time complexity of the most time efficient machine M deciding X is in O(t(n)). Similarly, Y is said to be harder to decide (or more complex) than X if the time complexity of X is asymptotically bounded by the time complexity of Y. The space complexity of a problem is defined similarly.</p></li></br></br></br>
				</ul>
				A complexity class can now be defined to be the set of problems for which there exists a decision procedure with a given running time or running space complexity. For instance, the class TIME(f(n)) denotes the class of problems with time complexity f(n). P – or polynomial time – is used to denote the union of the classes TIME(nk) for k∈N with respect to the reference model T. P hence subsumes all problems for which there exists a decision algorithm which can be implemented by a Turing machine whose time complexity is of polynomial order of growth. SPACE(f(n)) and PSPACE – or polynomial space – is defined similarly. Several other complexity classes we will consider below (e.g. NP, BPP, BQP) are defined by changing the reference model of computation, the definition of what it means for a machine to accept or reject an input, or both.</br></br></br>
				<h1><b>1.3 Distinguishing notions of complexity</b></h1></br></br></br>
				With these conventions in place, we can now record several respects in which the meaning assigned to the word ‘complexity’ in computational complexity theory differs from that which is assigned to this term in several other fields. In computational complexity theory, it is problems – i.e. infinite sets of finite combinatorial objects like natural numbers, formulas, graphs – which are assigned ‘complexities’. As we have just seen, such assignments are based on the time or space complexity of the most efficient algorithms by which membership in a problem can be decided. A distinct notion of complexity is studied in Kolmogorov complexity theory (e.g., Li and Vitányi 1997). Rather than studying the complexity of sets of mathematical objects, this subject attempts to develop a notion of complexity which is applicable to individual combinatorial objects – e.g. specific natural numbers, formulas, graphs, etc. For instance, the Kolmogorov complexity of a finite string x∈{0,1}∗ is defined to be the size of the smallest program for a fixed universal Turing machine which outputs x given the empty string as input. In this setting, the ‘complexity’ of an object is thus viewed as a measure of the extent to which its description can be compressed algorithmically.</br></br></br>
				Another notion of complexity is studied in descriptive complexity theory (e.g., Immerman 1999). Like computational complexity theory, descriptive complexity theory also seeks to classify the complexity of infinite sets of combinatorial objects. However, the ‘complexity’ of a problem is now measured in terms of the logical resources which are required to define its instances relative to the class of all finite structures for an appropriate signature. As we will see in Section 4.4 this approach often yields alternative characterizations of the same classes studied in computational complexity theory.</br></br>
				Yet another subject related to computational complexity theory is algorithmic analysis (e.g. Knuth (1973), Cormen, Leiserson, and Rivest 2005). Like computational complexity theory, algorithmic analysis studies the complexity of problems and also uses the time and space measures tM(n) and sM(x) defined above. The methodology of algorithmic analysis is different from that of computational complexity theory in that it places primary emphasis on gauging the efficiency of specific algorithms for solving a given problem. On the other hand, in seeking to classify problems according to their degree of intrinsic difficulty, complexity theory must consider the efficiency of all algorithms for solving a problem. Complexity theorists thus make greater use of complexity classes such as P,NP, and PSPACE whose definitions are robust across different choices of reference model. In algorithmic analysis, on the other hand, algorithms are often characterized relative to the finer-grained hierarchy of running times log2(n),n,n⋅log2(n),n2,n3,… within P.[7]</br></br></br>
			</p>
		</div>
		<div class="txtss2">
			<h2><b>2. The origins of complexity theory</b></h2></br></br></br>
			<h1><b>2.1 Church’s Thesis and effective computability</b></h1></br></br>
			<p>
				The origins of computational complexity theory lie in computability theory and early developments in algorithmic analysis. The former subject began with the work of Gödel, Church, Turing, Kleene, and Post originally undertaken during the 1930s in attempt to answer Hilbert’s Entscheidungsproblem – i.e. is the problem FO-VALID of determining whether a given formula of first-order logic is valid decidable? At this time, the concept of decidability at issue was that of effective decidability in principle – i.e. decidability by a rule governed method (or effective procedure) each of whose basic steps can be individually carried out by a finitary mathematical agent but whose execution may require an unbounded number of steps or quantity of memory space.</br></br></br>
				We now understand the Entscheidungsproblem to have been answered in the negative by Church (1936a) and Turing (1937). The solution they provided can be reconstructed as follows: 1) a mathematical definition of a model of computation M was presented; 2) an informal argument was given to show that M contains representatives of all effective procedures; 3) a formal argument was then given to show that no machine M∈M decides FO-VALID. Church (1936b) took M to be the class of terms Λ in the untyped lambda calculus, while Turing took M to correspond the class of T of Turing machines. Church also showed the class FΛ of lambda-definable functions is extensionally coincident with the class FR of general recursive functions (as defined by Gödel 1986b and Kleene 1936). Turing then showed that the class FT of functions computable by a Turing machine was extensionally coincident with FΛ.</br></br>
				The extensional coincidence of the classes FΛ,FR, and FT provided the first evidence for what Kleene (1952) would later dub Church’s Thesis – i.e.</br></br>
				(CT)	A function f:Nk→N is effectively computable if and only if f(x1,…,xk) is recursive.</br></br></br>
				CT can be understood to assign a precise epistemological significance to Church and Turing’s negative answer to the Entscheidungsproblem. For if it is acknowledged that FR (and hence also FΛ and FT) contain all effectively computable functions, it then follows that a problem X can be shown to be effectively undecidable – i.e. undecidable by any algorithm whatsoever, regardless of its efficiency – by showing that the characteristic function cX(x) of X is not recursive. CT thus allows us to infer from the fact that problems X for which cX(x) can be proven to be non-recursive – e.g. the Halting Problem (Turing 1937) or the word problem for semi-groups (Post 1947) – are not effectively decidable.</br></br></br>
				It is evident, however, that our justification for such classifications can be no stronger than the stock we place in CT itself. One form of evidence often cited in favor of the thesis is that the coincidence of the class of functions computed by the members of Λ,R and T points to the mathematical robustness of the class of recursive functions. Two related forms of inductive evidence are as follows: (i) many other independently motivated models of computation have subsequently been defined which describe the same class of functions; (ii) the thesis is generally thought to yield a classification of functions which has thus far coincided with our ability to compute them in the relevant ‘in principle’ sense.</br></br></br>
				But even if the correctness of CT is granted, it is also important to keep in mind that the concept of computability which it seeks to analyze is an idealized one which is divorced in certain respects from our everyday computational practices. For note that CT will classify f(x) as effectively computable even if it is only computable by a Turing machine T with time and space complexity functions tT(n) and sT(n) whose values may be astronomically large even for small inputs.[8]</br></br></br>
				Examples of this sort notwithstanding, it is often claimed that Turing’s original characterization of effective computability provides a template for a more general analysis of what it could mean for a function to be computable by a mechanical device. For instance, Gandy (1980) and Sieg (2009) argue that the process by which Turing originally arrived at the definition of a Turing machine can be generalized to yield an abstract characterization of a mechanical computing device. Such characterizations may in turn be understood as describing the properties which a physical system would have to obey in order for it to be concretely implementable. For instance the requirement that a Turing machine may only access or modify the tape cell which is currently being scanned by its read-write head may be generalized to allow modification to a machine’s state at a bounded distance from one or more computational loci. Such a requirement can in turn be understood as reflecting the fact that classical physics does not allow for the possibility of action at a distance.</br></br></br>
				On this basis CT is also sometimes understood as making a prediction about which functions are physically computable – i.e. are such that their values can be determined by measuring the states of physical systems which we might hope to use as practical computing devices. We might thus hope that further refinements of the Gandy-Sieg conditions (potentially along the lines of the proposals of Leivant (1994) or Bellantoni and Cook (1992) discussed in Section 4.5) will eventually provide insight as to why some mathematical models of computation appear to yield a more accurate account than others of the exigencies of concretely embodied computation which complexity theory seeks to analyze.</br></br></br>
				<h1><b>2.2 The Cobham-Edmond’s Thesis and feasible computability</b></h1></br></br></br>
				Church’s Thesis is often cited as a paradigm example of a case in which mathematical methods have been successfully employed to provide a precise analysis of an informal concept – i.e. that of effective computability. It is also natural to ask whether the concept of feasible computability described in Section 1 itself admits a mathematical analysis similar to Church’s Thesis.</br></br>
				We saw above that FACTORIZATION is an example of a problem of antecedent mathematical and practical interest for which more efficient algorithms have historically been sought. The task of efficiently solving combinatorial problems of the sort exemplified by TSP, INTEGER PROGRAMMING and PERFECT MATCHING grew in importance during the 1950s and 1960s due to their role in scientific, industrial, and clerical applications. At the same time, the availability of digital computers began to make many such problems mechanically solvable on a mass scale for the first time.</br></br>
				This era also saw several theoretical steps which heralded the attempt to develop a general theory of feasible computability. The basic definitions of time and space complexity for the Turing machine model were first systematically formulated by Hartmanis and Stearns (1965) in a paper called “On the Computational Complexity of Algorithms”. This paper is also the origin of the so-called Hierarchy Theorems (see Section 3.2) which demonstrate that a sufficiently large increase in the time or space bound for a Turing machine computation allows more problems to be decided.</br></br></br>
				A systematic exploration of the relationships between different models of computation was also undertaken during this period. This included variants of the traditional Turing machine model with additional heads, tapes, and auxiliary storage devices such as stacks. Another important model introduced at about this time was the random access (or RAM) machine A (see, e.g, Cook and Reckhow 1973). This model provides a simplified representation of the so-called von Neumann architecture on which contemporary digital computers are based. In particular, a RAM machine A consists of a finite sequence of instructions (or program) ⟨π1,…,πn⟩ expressing how numerical operations (typically addition and subtraction) are to be applied to a sequence of registers r1,r2,… in which values may be stored and retrieved directly by their index.</br></br></br>
				Showing that one of these models M1 determines the same class of functions as some reference model M2 (such as T) requires showing that for all M1∈M1, there exists a machine M2∈M2 which computes the same function as M1 (and conversely). This is typically accomplished by constructing M2 such that each of the basic steps of M1 is simulated by one or more basic steps of M2. Demonstrating the coincidence of the classes of functions computed by the models M1 and M2 thus often yields additional information about their relative efficiencies. For instance, it is generally possible to extract from the definition of a simulation between M1 and M2 time and space overhead functions ot(x) and os(x) such that if the value of a function f(x) can be computed in time t(n) and space s(n) by a machine M1∈M1, then it can also be computed in time ot(t(n)) and space os(s(n)) by some machine M2∈M2.</br></br></br>
				For a wide class of models, a significant discovery was that efficient simulations can be found. For instance, it might at first appear that the model A allows for considerably more efficient implementations of familiar algorithms than does the model T in virtue of the fact that a RAM machine can access any of its registers in a single step whereas a Turing machine may move its head only a single cell at a time. Nonetheless it can be shown that there exists a simulation of the RAM model by the Turing machine model with cubic time overhead and constant space overhead – i.e. ot(t(n))∈O(t(n)3) and os(s(n))∈O(s(n)) (Slot and Emde Boas 1988). On the basis of this and related results, Emde Boas (1990) formulated the following proposal to characterize the relationship between reference models which might be used for defining time and space complexity:</br></br>
				<b>Invariance Thesis</b> ‘Reasonable’ models of computation can simulate each other within a polynomially bounded overhead in time and a constant-factor overhead in space.</br></br></br>
				The 1960s also saw a number of advances in algorithmic methods applicable to problems in fields like graph theory and linear algebra. One example was a technique known as dynamic programming. This method can sometimes be used to find efficient solutions to optimization problems which ask us to find an object which minimizes or maximizes a certain quantity from a range of possible solutions. An algorithm based on dynamic programming solves an instance of such a problem by recursively breaking it into subproblems, whose optimal values are then computed and stored in a manner which can then be efficiently reassembled to achieve an optimal overall solution.</br></br></br>
				Bellman (1962) showed that the naive time complexity of O(n!) for TSP could be improved to O(2nn2) via the use of dynamic programming. The question thus arose whether it was possible to improve upon such algorithms further, not only for TSP, but also for other problem such as SAT for which efficient algorithms had been sought but were not known to exist. In order to appreciate what is at stake with this question, observe that the naive algorithm for TSP works as follows: 1) enumerate the set SG of all possible tours in G and compute their weights; 2) check if the cost of any of these tours is ≤b. Note, however, that if G has n nodes, then SG may contain as many as n! tours.</br></br></br>
				This is an example of a so-called brute force algorithm -- i.e. one which solves a problem by exhaustively enumerating all possible solutions and then successively testing whether any of them are correct. Somewhat more precisely, a problem X is said to admit a brute force solution if there exists a feasibly decidable relation RX and a family of uniformly defined finite sets Sx such that x∈X if and only if there exists a feasibly sized witness y∈Sx such that RX(x,y). Such a y is often called a certificate for x’s membership in X. The procedure of deciding x∈X by exhaustively searching through all of the certificates y0,y1,…∈Sx and checking if RX(x,y) holds at each step is known as a brute force search. For instance, the membership of a propositional formula ϕ with atomic letters among P0,…,Pn−1 in the problem SAT can be established by searching through the set Sϕ of possible valuation functions of type v:{0,…,n−1}→{0,1}, to determine if there exists v∈Sϕ such that [[ϕ]]v=1. Note, however, that since there are 2n functions in Sϕ, this yields only an exponential time decision algorithm.</br></br></br>
				Many other problems came to light in the 1960s and 1970s which, like SAT and TSP, can easily be seen to possess exponential time brute force algorithms but for which no polynomial time algorithm could be found. On this basis, it gradually came to be accepted that a sufficient condition for a decidable problem to be intractable is that the most efficient algorithm by which it can be solved has at best exponential time complexity. The corresponding positive hypothesis that possession of a polynomial time decision algorithm should be regarded as sufficient grounds for regarding a problem as feasibly decidable was first put forth by Cobham (1965) and Edmonds (1965a).</br></br></br>
				Cobham began by citing the evidence motivating the Invariance Thesis as suggesting that the question of whether a problem admits a polynomial time algorithm is independent of which model of computation is used to measure time complexity across a broad class of alternatives. He additionally presented a machine-independent characterization of the class FP – i.e. functions f:Nk→N which are computable in polynomial time – in terms of a restricted form of primitive recursive definition known as bounded recursion on notation</br></br></br>
				Edmonds (1965a) first proposed that polynomial time complexity could be used as a positive criterion of feasibility – or, as he put it, possessing a “good algorithm” – in a paper in which he showed that a problem which might a priori be thought to be solvable only by brute force search (a generalization of PERFECT MATCHING from above) was decidable by a polynomial time algorithm. Paralleling a similar study of brute force search in the Soviet Union, in a subsequent paper Edmonds (1965b) also provided an informal description of the complexity class NP. In particular, he characterized this class as containing those problems X for which there exists a “good characterization” – i.e. X is such that the membership of an instance x may be verified by using brute force search to find a certificate y of feasible size which certifies x’s membership in X.[9]</br></br></br>
				These observations provided the groundwork for has come to be known as the Cobham-Edmonds Thesis (see, e.g., Brookshear et al. 2006; Goldreich 2010; Homer and Selman 2011):</br></br>
				(CET)	A function f:Nk→N is feasibly computable if and only if f(x⃗ ) is computed by some machine M such that tM(n)∈O(nk) where k is fixed and M is drawn from a reasonable model of computation M.</br></br></br>
				
			</p>
		</div>
		<div class="txtss3">
			<h2><b>3. Technical development</b></h2></br></br></br>
			<h1><b>3.1 Deterministic and non-deterministic models of computation</b></h1></br></br></br>
			<p>
				According to the Cobham-Edmonds Thesis the complexity class P describes the class of feasibily decidable problems. As we have just seen, this class is defined in terms of the reference model T in virtue of the assumption that it is a ‘reasonable’ model of computation. Several other models of computation are also studied in complexity theory not because they are presumed to be accurate representations of the costs of concretely embodied computation, but rather because they help us better understand the limits of feasible computability. The most important of these is the non-deterministic Turing machine model N.</br></br></br>
				Recall that a deterministic Turing machine T∈T can be represented as a tuple ⟨Q,Σ,δ,s⟩ where Q is a finite set of internal states, Σ is a finite tape alphabet, s∈Q is T’s start state, and δ is a transition function mapping state-symbol pairs ⟨q,σ⟩ into state-action pairs ⟨q,a⟩. Here a is chosen from the set of actions {σ,⇐,⇒} – i.e. write the symbol σ∈Σ on the current square, move the head left, or move the head right. Such a function is hence of type δ:Q×Σ→Q×α. On the other hand, a non-deterministic Turing machine N∈N is of the form ⟨Q,Σ,Δ,s⟩ where Q,Σ, and s are as before but Δ is now only required to be a relation – i.e. Δ⊆(Q×Σ)×(Q×α). As a consequence, a machine configuration in which N is in state q and reading symbol σ can lead to finitely many distinct successor configurations – e.g. it is possible that Δ relates ⟨q,σ⟩ to both ⟨q′,a′⟩ and ⟨q′′,a′′⟩ for distinct states q′ and q′′ and actions a′ and a′′.[11]</br></br></br>
				This difference in the definition of deterministic and non-deterministic machines also necessitates a change in the definition of what it means for a machine to decide a language X. Recall that for a deterministic machine T, a computation sequence starting from an initial configuration C0 is a finite or infinite sequence of machine configurations C0,C1,C2,… Such a configuration consists of a specification of the contents of T’s tape, internal state, and head position. Ci+1 is the unique configuration determined by applying the transition function δ to the active state-symbol pair encoded by Ci and is undefined if δ is itself undefined on this pair (in which case the computation sequence is finite, corresponding to a halting computation). If N is a non-deterministic machine, however, there may be more than one configuration which is related to the current configuration by Δ at the current head position. In this case, a finite or infinite sequence C0,C1,C2,… is said to be a computation sequence for N from the initial configuration C0 just in case for all i≥0, Ci+1 is among the configurations which are related by Δ to Ci (and is similarly undefined if no such configuration exists).</br></br></br>
				We now also redefine what is required for the machine N to decide a language X:</br></br>
				N always halts – i.e. for all initial configurations C0, the computation sequence C0,C1,C2,… of N is of finite length;</br></br>
				if x∈X and C0(x) is the configuration of N encoding x as input, then there exists a computation sequence C0(x),C1(x),…,Cn(x) of N such that Cn(x) is an accepting state;</br></br>
				if x∉X, then all computation sequences C0(x),C1(x),…,Cn(x) of N are such that Cn(x) is a rejecting state.</br></br></br>
				Note that this definition treats accepting and rejecting computations asymmetrically. For if x∈X, some of N’s computation sequences starting from C0(x) may still lead to rejecting states as long as it least one leads to an accepting state. On the other hand, if x∉X, then all of N’s computations from C0(x) are required to lead to rejecting states.</br></br></br>
				Non-deterministic machines are sometimes described as making undetermined ‘choices’ among different possible successor configurations at various points during their computation. But what the foregoing definitions actually describe is a tree TNC0 of all possible computation sequences starting from a given configuration C0 for a deterministic machine N (an example is depicted in Figure 1). Given the asymmetry just noted, it will generally be the case that all of the branches in TNC0(x) must be surveyed in order to determine N’s decision about the input x.</br></br></br>
				The time complexity tN(n) of a non-deterministic machine N is the maximum of the depths of the computation trees TNC0(x) for all inputs x such that |x|=n. Relative to this definition, non-deterministic machines can be used to implement many brute force algorithms in time polynomial in n. For instance the SAT problem can be solved by a non-deterministic machine which on input ϕ uses part of its tape to non-deterministically construct (or ‘guess’) a string representing a valuation v assigning a truth value to each of ϕ’s n propositional variables and then computes [[ϕ]]v using the method of truth tables (which is polynomial in n). As ϕ∈SAT just in case a satisfying valuation exists, this is a correct method for deciding SAT relative to conventions (i)–(iii) from above. This means that SAT can be solved in polynomial time relative to N.</br></br></br>
				This example also illustrates why adding non-determinism to the original deterministic model T does not enlarge the class of decidable problems. For if N∈N decides X, then it is possible to construct a machine TN∈T which also decides X, by successively simulating each of the finitely many possible sequences of non-deterministic choices N might have made in the course of its computation.[12] It is evident that if N has time complexity f(n), then TN must generally check O(kf(n)) sequences of choices (for fixed k) in order to determine the output of N for an input of length n. While the availability of such simulations shows that the class of languages decided by N is the same as that decided by T – i.e. exactly the recursive ones – it also illustrates why the polynomial time decidability of a language by a non-deterministic Turing machine only guarantees that the language is decidable in exponential time by a deterministic Turing machine.</br></br></br>
				In order to account for this observation, Emde Boas (1990) introduced a distinction between two classes of models of computation which he labels the first and second machine classes. The first machine class contains the basic Turing machine model T as well as other models which satisfy the Invariance Thesis with respect to this model. As we have seen, this includes the multi-tape and multi-head Turing machine models as well as the RAM model. On the other hand, the second machine class is defined to include those deterministic models whose members can be used to efficiently simulate non-deterministic computation. This can be shown to include a number of standard models of parallel computation such as the PRAM machine which will be discussed in Section 3.4.3. For such models the definitions of polynomial time, non-deterministic polynomial time, and polynomial space coincide.</br></br></br>
				Experience has borne out that members of the first machine class are the ones which we should consider reasonable models of computation in the course of formulating the Cobham-Edmonds Thesis. It is also widely believed that members of the second machine class do not provide realistic representations of the complexity costs involved in concretely embodied computation (Chazelle and Monier (1983), Schorr (1983), Vitányi (1986)). Demonstrating this formally would, however, require proving separation results for complexity classes which are currently unresolved. Thus while it is widely believed that the second machine class properly extends the first, this is currently an open problem.</br></br></br>
				
			</p>
			<h1><b>3.2 Complexity classes and the hierarchy theorems</b></h1></br></br></br>
			<p>
				Recall that a complexity class is a set of languages all of which can be decided within a given time or space complexity bound t(n) or s(n) with respect to a fixed model of computation. To avoid pathologies which would arise were we to define complexity classes for ‘unnatural’ time or space bounds (e.g. non-recursive ones) it is standard to restrict attention to complexity classes defined when t(n) and s(n) are time or space constructible. t(n) is said to be time constructible just in case there exists a Turing machine which on input consisting of 1n (i.e. a string of n 1s) halts after exactly t(n) steps. Similarly, s(n) is said to be space constructible just in case there exists a Turing machine which on input 1n halts after having visited exactly s(n) tape cells. It is easy to see that the time and space constructible functions include those which arise as the complexities of algorithms which are typically considered in practice – log(n),nk,2n,n!, etc.</br></br></br>
				When we are interested in deterministic computation, it is conventional to base the definitions of the classical complexity classes defined in this section on the model T. Supposing that t(n) and s(n) are respectively time and space constructible functions, the classes TIME(t(n)) and SPACE(s(n)) are defined as follows:</br></br>
				TIME(t(n))={X⊆{0,1}∗ : ∃SPACE(s(n))={X⊆{0,1}∗ : ∃T∈T∀n(timeT(n)≤t(n))</br></br>
				Since all polynomials in the single variable n are of order O(nk) for some k, the classes known as polynomial time and polynomial space are respectively defined as P=⋃k∈NTIME(nk) and PSPACE=⋃k∈NSPACE(nk). It is also standard to introduce the classes EXP=⋃k∈NTIME(2nk) (exponential time) and L=SPACE(log(n)) (logarithmic space).</br></br>
				n addition to classes based on the deterministic model T, analogous non-deterministic complexity classes based on the model N are also studied. In particular, the classes NTIME(t(n)) and NSPACE(s(n)) are defined as follows:</br></br>
				NTIME(t(n))={X⊆{0,1}∗ : ∃NSPACE(s(n))={X⊆{0,1}∗ : ∃N∈N∀n(timeN(n)≤t(n))and N decides X}N∈N∀n(spaceN(n)≤s(n))and N decides X}</br></br></br>
				The classes NP (non-deterministic polynomial time), NPSPACE (non-deterministic polynomial space), NEXP (non-deterministic exponential time), and NL (non-deterministic logarithmic space) are defined analogously to P, NP, EXP and L – e.g. NP=⋃k∈NNTIME(nk).</br></br></br>
				Many classical results and important open questions in complexity theory concern the inclusion relationships which hold among these classes. Central among these are the so-called Hierarchy Theorems which demonstrate that the classes TIME(t(n)) form a proper hierarchy in the sense that if t2(n) grows sufficiently faster than t1(n), then TIME(t2(n)) is a proper superset of TIME(t1(n)) (and similarly for NTIME(t(n)) and SPACE(s(n))).</br></br></br>
			</p>
		</div>
		</section>
		<section id="computability">
			<div class="background">
				<div class="glass-effect">
				</div>
				<div class="topic-title">
					<small>Theory Of Computation</small>
					<h2>Computability Theory</h2>
					<p>Defines whether a problem is “solvable” by any abstract machine. Some problems are computable while others are not. Computation is done by various computation models depending on the nature of the problem at hand, examples of these machines are: the Turing machine, Finite state machines, and many others.</p></br>
				</div>
			</div>
			<div class="txtss">
				<center><h2><b>Computability theory</b></h2></center></br></br>
				<p>	
					is the area of mathematics dealing with the concept of an effective procedure—a procedure that can be carried out by following specific rules. For example, one might ask whether there is some effective procedure—some algorithm—that, given a sentence about the positive integers, will decide whether that sentence is true or false. In other words, is the set of true sentences about the positive integers decidable? Or for a much simpler example, the set of prime numbers is certainly a decidable set. That is, there are mechanical procedures, that are taught in the schools, for deciding of any given positive integer whether or not it is a prime number.</br></br></br></br></br></br>
					More generally, consider a set S, which can be either a set of natural numbers (the natural numbers are 0, 1, 2, … ), or a set of strings of letters from a finite alphabet. (These two situations are entirely interchangeable. A set of natural numbers is much like a set of base-10 numerals, which are strings of digits. And in the other direction, a string of letters can be coded by a natural number in a variety of ways. The best way is, where the alphabet has k symbols, to utilize k -adic notation, which is like base-k numerals except that the k digits represent 1, 2, …, k, without a 0 digit.) One can say that S is a decidable set if there exists an effective procedure that, given any natural number (in the first case) or string of letters (in the second case), will eventually end by supplying the answer: "Yes" if the given object is a member of S and "No" if it is not a member of S.</br></br></br></br></br></br>
					And by an effective procedure here is meant a procedure for which one can give exact instructions—a program—for carrying out the procedure. Following these instructions should not demand brilliant insights on the part of the agent (human or machine) following them. It must be possible, at least in principle, to make the instructions so explicit that they can be executed by a diligent clerk (who is good at following directions but is not too clever) or even a machine (which does not think at all).</br></br>
					Although these instructions must of course be finite in length, no upper bound on their possible length is imposed. It is not ruled out that the instructions might even be absurdly long. Similarly, to obtain the most comprehensive concepts, no bounds are imposed on the time that the procedure might consume before it supplies the answer. Nor is a bound imposed on the amount of storage space (scratch paper) that the procedure might need to use. One merely insists that the procedure give an answer eventually, in some finite length of time.</br></br></br></br></br></br>
					This description of effective procedures, vague as it is, already shows how limiting the concept of decidability is. It is not hard to see that there are only countably many possible instructions of finite length that one can write out (using a standard keyboard, say). There are, however, uncountably many sets of natural numbers (by Cantor's diagonal argument). It follows that almost all sets, in a sense, are undecidable.</br></br></br></br>
					The following section will look at how the foregoing vague description of effective procedures can be made more precise—how it can be made into a mathematical concept. Nonetheless, the informal idea of what can be done by effective procedure, that is, what is calculable, can be useful.</br></br>
					For another example, consider what is required for a string of symbols to constitute an acceptable mathematical proof. Before one accepts a proof and adds the result being proved to the storehouse of mathematical knowledge, one insists that the proof be verifiable. That is, it should be possible for another mathematician, such as the referee of the paper containing the proof, to check, step by step, the correctness of the proof. Eventually, the referee concludes either that the proof is indeed correct or that the proof contains a gap or an error and is not yet acceptable. That is, the set of acceptable mathematical proofs should be decidable. This fact will be seen (in section 4) to have significant consequences for what can and cannot be proved. The conclusion follows that computability theory is relevant to the foundations of mathematics.</br></br></br></br></br></br>
					Before going on, one should broaden the canvas from considering decidable and undecidable sets to considering the more general situation of partial functions. Let U be either the set ℕ = {0,1,2, … } of natural numbers or the set Σ* of all strings of letters—all words—from a finite alphabet Σ. Then a k -place partial function on U is a function whose domain is included in Uk = U × U × … × U and whose range is included in U. And one can say that such a function is total if its domain is all of Uk.</br></br></br></br>
				</p>
				<h1><b>For a k -place partial function f, one can say that f is an effectively calculable partial function if there exists an effective procedure with the following property:</b></h1></br></br>
				<ul>
					<li><p>Given a k -tuple x in the domain of f, the procedure eventually halts and returns the correct value for f (x )</p></li></br></br>
					<li><p>Given a k -tuple x not in the domain of f, the procedure does not halt and return a value</p></li></br></br></br>
				</ul>
				<p>
					(Strictly speaking, when U is ℕ, the procedure cannot be given numbers, it must be given numerals. Numerals are bits of language, which can be communicated. Numbers are not. Thus, the difference between U = ℕ and U = Σ* is even less than previously indicated.)</br></br></br>
				</p>
				<h1><b>For example, the partial function for subtraction</b></h1></br></br>
				<p>
					(where ↑ indicates that the function is undefined) is effectively calculable, and procedures for calculating it, using base-10 numerals, are taught in the elementary schools.</br></br>
					The concept of decidability can then be described in terms of functions: For a subset S of Uk, one can say that S is decidable if its characteristic function</br></br>
					(which is always total) is effectively calculable. Here, "Yes" and "No" are some fixed members of U, such as 1 and 0 in the case of ℕ.</br></br>
					Here, if k = 1, then S is a set of numbers or a set of words. If k = 2, then one has the concept of a decidable binary relation on numbers or words, and so forth.</br></br>
					And it is natural to extend this concept to the situation where one has half of decidability: Say that S is semidecidable if its partial characteristic function is an effectively calculable partial function. Thus, a set S of words—a language—is semidecidable if there is an effective procedure for recognizing members of S. One can think of S as the language that the procedure accepts.</br></br></br></br></br></br>
		
				</p>
				<h1><b>The following is another example of a calculable partial function:</b></h1></br></br>
				<p>F (n ) = the smallest p > n such that both p and p + 2 are prime Here, it is to be understood that F (n ) is undefined if there is no number p as described; thus F might not be total. For example, F (9) = 11. It is not known whether or not F is total. Nonetheless, one can be certain that F is effectively calculable. One procedure for calculating F (n ) proceeds as follows. "Given n, first put p = n + 1. Then check whether or not p and p + 2 are both prime. If they are, then stop and give output p. If not, increment p and continue." What if n = 101000? On the one hand, if there is a larger prime pair, then this procedure will find the first one, and halt with the correct output. On the other hand, if there is no larger prime pair, then the procedure never halts, so it never gives an answer. That is all right, because F (n ) is undefined—the procedure should not give any answer. (Of course, F is total if and only if (iff) the twin prime conjecture is true.)</p></br></br></br></br>
				<h1><b>Now suppose one modifies this example. Consider the total function:</b></h1></br></br></br>
				<p>
					Here, F (n ) ↓ means that F (n ) is defined so that n belongs to the domain of F. Then the function G is also effectively calculable. That is, there exists a program that calculates G correctly. That is not the same as saying that one knows that program. This example indicates the difference between knowing that a certain effective procedure exists and having the effective procedure in one's hands.</br></br>
					One person's program is another person's data. This is the principle behind operating systems (and behind the idea of a stored-program computer). One's favorite program is, to the operating system, another piece of data to be received as input and processed. The operating system is calculating the values of a two-place "universal" function, as in the following example.</br></br>
					Suppose one adopts a fixed method of encoding any set of instructions by a single natural number. (First, one converts the instructions to a string of 0s and 1s—one always does this with computer programs—and then one regards that string as naming a natural number under a suitable base-2 notation.) Then, the universal function Φ(x, y ) = the result of applying the instructions coded by y to the input x is an effectively calculable partial function (where it is understood that Φ(x, y ) is undefined whenever applying the instructions coded by y to the input x fails to halt and return an output). Here are the instructions for Φ: "Given x and y, decode y to see what it says to do with x, and then do it." Of course, the function Φ is not total.</br></br></br>
				</p>
				<h1><b>Using this universal partial function, one can construct an undecidable binary relation, the halting relation H :</b></h1></br></br></br>
				<p>
					(x,y) ∈H ⇔ Φ (x,y)↓ ⇔ applying the instructions coded byy to input x halts</br></br>
					To see that H is undecidable, one can argue as follows. Suppose that, to the contrary, H is decidable. Then the following function would be effectively calculable:</br></br></br>
					(Notice the use of the classical diagonal construction.) (To compute f (x ), one first would decide if (x, x ) ∈ H. If not, then f (x ) = Yes. If (x, x ) ∈ H, however, then the procedure for finding f (x ) should throw itself into an infinite loop, because f (x ) is undefined.) The function f cannot possibly be effectively calculable, however. Consider any set of instructions that might compute f. Those instructions have some code number k, but f has been constructed in such a way that f (k ) differs from the output from the result of applying instructions coded by k to the input k. (They differ because one is defined and one is not.) So these instructions cannot correctly compute f ; they produce the wrong result at the input k. And so one has a contradiction. That the previous relation H is undecidable is usually expressed by saying that "the halting problem is unsolvable"; that is, one cannot effectively determine, given x and y, whether applying the instructions coded by y to the input x will eventually terminate or will go on forever.</br></br></br>
					While the concept of effective calculability has been described in somewhat vague terms here, the following section will give a precise (mathematical) concept of a computable partial function. And then it will be argued that the mathematical concept of a computable partial function is the correct formalization of the informal concept of an effectively calculable partial function. This claim is known as Church's thesis or the Church-Turing thesis. Church's thesis, which relates an informal idea to a formal idea, is not itself a mathematical statement, capable of being given a proof, but one can look for evidence for or against Church's thesis; it all turns out to be evidence in favor.</br></br></br></br></br></br>
					One piece of evidence is the absence of counterexamples. That is, any function examined thus far that mathematicians have felt was effectively calculable, has been found to be computable.</br></br></br></br>
					Stronger evidence stems from the various attempts that different people made independently, trying to formalize the idea of effective calculability. Alonzo Church used λ-calculus, Alan M. Turing used an idealized computing agent (later called a Turing machine), and Emil Post developed a similar approach. Remarkably, all these attempts turned out to be equivalent, in that they all defined exactly the same class of functions, namely, the computable partial functions!</br></br>Stronger evidence stems from the various attempts that different people made independently, trying to formalize the idea of effective calculability. Alonzo Church used λ-calculus, Alan M. Turing used an idealized computing agent (later called a Turing machine), and Emil Post developed a similar approach. Remarkably, all these attempts turned out to be equivalent, in that they all defined exactly the same class of functions, namely, the computable partial functions!</br></br></br></br></br></br>
				</p>
			</div>
			<div class="txtss2">
				<p>The study of effective calculability originated in the 1930s with work in mathematical logic. As noted earlier, the subject is related to the concept on an acceptable proof. Since the development of modern computers the study of effective calculability has formed an essential part of theoretical computer science. A prudent computer scientist would surely want to know that, apart from the difficulties the real world presents, there is a purely theoretical limit to calculability.</p></br></br></br>
				<h2><b>1. Feasible Computability</b></h2></br></br></br>
				<p>
					Up to now, this entry has approached computability from the point of view that there should be no constraints on the time required for a particular computation, or on the amount of memory space that might be required. The result is that some total computable functions will take a long time to compute. If a function f grows rapidly, then for large x it will take a long time simply to generate the output f (x ). There are also, however, bounded functions that require a large amount of time.</br></br></br></br>
					To be more precise, suppose one adopts one of the formalizations from section 1 (any one will do), and one defines in a reasonable way the "number of steps" or the "time required" in a computation. (Manuel Blum converted the term reasonable into axioms for what a complexity measure should be.) Then Michael Rabin showed that for any total computable function h, no matter how fast it grows, one can find another total computable function f with range {0, 1} such that for any program e for f (i.e., f = φe ), the time required for e to compute f (x ) exceeds h (x ) for all but finitely many values of x. (The function f is constructed in stages, in such a way as to sabotage any fast program that might try to compute f.)</br></br></br></br></br></br>
					Is there a more restricted concept of "feasibly computable function" where the amount of time required does not grow beyond all reason, where the amount of time required is an amount that might actually be practical, at least when the input to the function is not absurdly large? To this vague question, an exact answer has been proposed.</br></br></br></br>
					Once can call a function f polynomial-time computable (or for short, P-time computable) if there is a program e for f and a polynomial p such that for every x, the program e computes f (x ) in no more than p (│x │) steps, where │x │ is the length of x.</br></br></br>
					This definition requires some explanation and support. If f is a function over Σ*, the set of words over a finite alphabet Σ, then of course │x │ is just the number of symbols in the word x. If f is a function over ℕ, then │x │ is the length of the numeral for x. (Here, one comes again to the fact that effective procedures work with numerals, not numbers.) So if one uses base-2 numerals for ℕ, then │x │ is about log2x.</br></br></br></br></br></br>
					Moreover, there was vagueness about exactly how the number of steps in a computation was to be determined. Here the situation is encouraging: The class of P-time computable functions is the same, under the different reasonable ways of counting steps.</br></br></br></br></br></br>
					Back in sections 0 and 1 there was the encouraging fact that many different ways of formalizing the concept of effective calculability yielded exactly the same class of functions. As remarkable as that fact is, even more is true. The number of steps required by one formalization is bounded by a polynomial in the number of steps required by another. For example, there exists a polynomial p (of moderate degree) such that a computation by a Turing machine that requires n steps can be simulated by a loop-while program that requires no more than p (n ) steps. Consequently, the concept of a P-time computable function is robust: One can get the same class of functions, regardless of which formalization from section 1 is employed. To be sure, the degrees of the polynomials will vary somewhat, but the class of P-time functions is unchanged.</br></br></br></br></br></br>
					Encouraged by this result, and inspired in particular by 1971 work of Stephen Cook, people since the 1970s have come to regard the class of P-time functions as the correct formalization of the idea of functions for which computations are feasible, without totally impractical running times.</br></br></br></br></br>
					By analogy to Church's thesis, the statement that P-time computability corresponds to feasibly practical computability has come to be known as Cook's thesis or the Cook-Karp thesis. (The concept of P-time computability appeared as early as 1964 in work of Alan Cobham. Jack Edmunds in 1965 pointed out the good features of P-time algorithms. Richard Karp in 1972 extended Cook's work.)</br></br></br></br></br></br>
					So what are the P-time computable functions? They form a subclass of the primitive recursive functions. All the polynomial functions are P-time computable, as are some functions that grow faster than any polynomial. There is, however, a limit to the growth rate of P-time computable functions, imposed by the fact that printing an output symbol takes a step. That is, there is the following growth limitation property: If f is computable in time bounded by the polynomial p, then │f (x )│ ≤ │x │ + p (│x │). This prevents exponential functions from being P-time computable; there is not enough time to write down the result.</br></br></br></br></br></br>
					Often, P-time computability is presented in terms of acceptance of languages (i.e., sets of words). Where Σ is the finite alphabet in question, consider a language L ⊆ Σ*. One can say that L ∈ P if there is a program and a polynomial p such that whenever a word w is in L, then the program halts on input w (i.e., it "accepts" w ) in no more than p (│w │) steps, and whenever a word w is not in L, then the program never halts on input w (i.e., the program does not accept w ). This is equivalent to saying that the characteristic function of L is P-time computable, because one can add to the program an alarm clock that rings after time p (│w │). For example, it is now known that the set of prime numbers (as a set of words written in the usual base-10 notation) belongs to P.</br></br></br>
					Of course, if the characteristic function of L is P-time computable, then so is the characteristic function of its complement, L̄. That is, P = co-P, where co-P is the collection of complements of languages in P.</br></br></br></br></br></br>
					Informally, L is in P if L is not only a decidable set of words, but moreover there is a fast decision procedure for P—one that can actually be implemented in a practical way. For example, finite graphs can be coded by words over a suitable finite alphabet. The set of two-colorable graphs (i.e., the set of graphs that can be properly colored with two colors) is in P, because it is fast to check that the graph has no cycles of odd length. The set of graphs with an Euler cycle is in P, because it is fast to check that the graph is connected and that every vertex has even degree.</br></br></br>

				</p>
				<h2><b>2. Formalizations</b></h2></br></br></br>
				<p>
					In the preceding section, the concept of effective calculability was described only informally. Now, these ideas will be made more precise (i.e., will be made part of mathematics). In fact, several approaches to doing this will be described: idealized computing devices, generative def-initions (i.e., the least class containing certain initialfunctions and closed under certain constructions), programming languages, and definability in formal languages. It is a significant fact that these different approaches all yield exactly equivalent concepts.</br></br></br></br></br></br>
				</p>
				<h1><b>Turing machines</b></h1></br></br></br>
				<p>
					In early 1935 Alan M. Turing was a twenty-two-year-old graduate student at King's College in Cambridge. Under the guidance of Max Newman, he was working on the problem of formalizing the concept of effective calculability. In 1936 he learned of the work of Alonzo Church at Princeton University. Church had also been working on this problem, and in his 1936 paper "An Unsolvable Problem of Elementary Number Theory" he presented a definite conclusion: that the class of effectively calculable functions should be identified with the class of functions definable in the λ-calculus, a formal language for specifying the construction of functions. Moreover, he showed that exactly the same class of functions could be characterized in terms of formal derivability from equations.</br></br></br></br></br></br>
					Turing then promptly completed writing his paper, in which he presented a different approach to characterizing the effectively calculable functions, but one that—as he proved—yielded once again the same class of functions as Church had proposed. With Newman's encouragement, Turing then went to Princeton for two years, where he wrote a doctoral dissertation under Church.</br></br></br>
					Turing's paper remains a readable introduction to his ideas. How might a diligent clerk carry out a calculation, following instructions? He might organize his work in a notebook. At any given moment his attention is focused on a particular page. Following his instructions, he might alter that page, and then he might turn to another page. And the notebook is large enough that he never comes to the last page.</br></br></br>
					The alphabet of symbols available to the clerk must be finite; if there were infinitely many symbols, then there would be two that were arbitrarily similar and so might be confused. One can then without loss of generality regard what can be written on one page of notebook as a single symbol. And one can envision the notebook pages as being placed side by side, forming a paper tape, consisting of squares, each square being either blank or printed with a symbol. At each stage of his work, the clerk—or the mechanical machine—can alter the square under examination, can turn attention to the next square or the previous one, and can look to the instructions to see what part of them to follow next. Turing described the latter part as a "change of state of mind."</br></br></br></br></br></br>
					Turing wrote, "We may now construct a machine to do the work" (1936–1937, p. 251). Of course, such a machine is now called a Turing machine, a phrase first used by Church in his review of Turing's paper in The Journal of Symbolic Logic. The machine has a potentially infinite tape, marked into squares. Initially, the given input numeral or word is written on the tape, but it is otherwise blank. The machine is capable of being in any one of finitely many states (the phrase "of mind" being inappropriate for a machine). At each step of calculation, depending on its state at the time, the machine can change the symbol in the square under examination at that time, can turn its attention to the square to the left or to the right, and can then change its state to another state.</br></br></br>
					The program for this Turing machine can be given by a table. Where the possible states of the machine are q 1, …, qr, each line of the table is a quintuple 〈qi, Sj, Sk, D, qm 〉, which is to be interpreted as directing that whenever the machine is in state qi and the square under examination contains the symbol Sj, then that symbol should be altered to Sk and the machine should shift its attention to the square on the left (if D = L ) or on the right (if D = R ), and should change its state to qm. For the program to be unambiguous, it should have no two different quintuples with the same first two components. (By relaxing this requirement regarding absence of ambiguity, one obtains the concept of a nondeterministic Turing machine, which will be useful later, in the discussion of feasible computability.) One of the states, say q 1, is designated as the initial state—the state in which the machine begins its calculation. If one starts the machine running in this state and examining the first square of its input, it might (or might not), after some number of steps, reach a state and a symbol for which its table lacks a quintuple having that state and symbol for its first two components. At that point the machine halts, and one can look at the tape (starting with the square then under examination) to see what the output numeral or word is.</br></br></br></br></br></br>
					Now suppose that Σ is a finite alphabet and that f is a k -place partial function on the set Σ* of words. One says that f is Turing computable if there exists a Turing machine M that, when started in its initial state scanning the first symbol of a k -tuple w⃗ of words (written on the tape, with a blank square between words, and with everything to the right of w⃗ blank), behaves as follows:</br></br></br>
			
				</p>
				<ul>
					<li><p>If f (w⃗ ) ↓ (i.e., if w⃗ ∈ dom f ), then M eventually halts, and at that time it is scanning the leftmost symbol of the word f (w⃗ ) (which is followed by a blank square).</p></li></br></br>
					<li><p>If f (w⃗ ) ↑ (i.e., if w⃗ ∉ dom f ), then M never halts.</p></li></br></br>
				</ul>
				<p>
					This definition can be readily adapted to apply to k -place partial functions on ℕ.</br></br></br>
					Then Church's thesis, also called—particularly in the context of Turing machines—the Church-Turing thesis, is the claim that this concept of Turing computability is the correct formalization of the informal concept of effective calculability. Certainly, the definition reflects the ideas of following predetermined instructions, without limitation of the amount of time that might be required. (The name Church-Turing thesis obscures the fact that Church and Turing followed different paths in reaching equivalent conclusions.)</br></br></br></br></br></br>
					As will be explained shortly, Church's thesis has by now achieved universal acceptance. Kurt Gödel, writing in 1964 about the concept of a formal system in logic, involving the idea that the set of correct deductions must be a decidable set, said that "due to A. M. Turing's work, a precise and unquestionably adequate definition of the general concept of formal system can now be given" (Davis 1965, p. 71).</br></br></br>
					The robustness of the concept of Turing computability is evidenced by the fact that it is insensitive to certain modifications to the definition of a Turing machine. For example, one can impose limitations on the size of the alphabet, or one can insist that the machine never move to the left of its initial starting point. None of this will affect that class of Turing computable partial functions.</br></br></br>
					Turing developed these ideas before the introduction of modern digital computers. After World War II he played an active role in the development of early computers and in the emerging field of artificial intelligence. (During the war, he worked on deciphering the German battlefield code Enigma, work that remained classified until after Turing's death.) One can speculate whether Turing might have formulated his ideas somewhat differently, if his work had come after the introduction of digital computers.</br></br></br></br></br></br>
					
				</p>
				<h1><b>primitive recursiveness and minimalization</b></h1></br></br></br>
				<p>
					For a second formalization of the calculability concept, a certain class of partial functions on ℕ will now be defined as the smallest class that contains certain initial function and is closed under certain constructions.</br></br></br>
					For the initial functions, one can take the following simple total functions:</br></br></br>
				</p>
				<ul>
					<li><p>The zero functions, that is, the constant functions f defined by the equation:</br>
f (x 1, …, xk ) = 0</p></li></br></br>
					<li><p>The successor function S, defined by the equation:</br>
S (x ) = x + 1</p></li></br></br>
					<li><p>The projection functions Ikn from k -dimensions onto the n th coordinate,</p></li></br>
				</ul>
				<p style="border-left:solid gray; color:gray">where 1 ≤ n ≤ k.</p></br></br></br>
				<p>
					One can form the closure of the class of initial functions under three constructions: composition, primitive recursion, and minimalization.</br></br></br></br></br></br>
					A k -place function h is said to be obtained by composition from the n -place function f and the k -place functions g 1, …, gn if the equation
h (x⃗ )=f (g 1)(x⃗ ),…,g n(x⃗ ))</br>
holds for all x⃗. In the case of partial functions, it is to be understood here that h (x⃗ ) is undefined unless g 1(x⃗ ), …, gn (x⃗ ) are all defined and 〈g 1(x⃗ ), …, gn (x⃗ )〉 belongs to the domain of f.</br></br></br>
A (k + 1)-place function h is said to be obtained by primitive recursion from the k -place function f and the (k + 2)-place function g (where k > 0) if the pair of equations
h (x⃗, 0) = f (x⃗ )</br>
h (x⃗t +1) = g(t, h (x⃗, t ), x⃗ )</br>
holds for all x⃗ and t.</br></br></br>
				Again, in the case of partial functions, it is to be understood that h (x⃗, t + 1) is undefined unless h (x⃗, t ) is defined and 〈t, h (x⃗, t ), x⃗  〉 is in the domain of g.</br></br></br>
				For the k = 0 case, the one-place function h is obtained by primitive recursion from the two-place function g with the number m if the pair of equations
h (0) = m</br>
h (t + 1) = g (t, h (t ))</br>
holds for all t.</br></br></br></br></br></br>
				Postponing the matter of minimalization, one can define a function to be primitive recursive if it can be built up from zero, successor, and projection functions by use of composition and primitive recursion. In other words, the class of primitive recursive functions is the smallest class that includes the initial functions and is closed under composition and primitive recursion.</br></br></br>
				Clearly, all the primitive recursive functions are total. One can say that a k -ary relation R on ℕ is primitive recursive if its characteristic function is primitive recursive.</br></br></br>
				One can then show that a great many of the common functions on ℕ are primitive recursive: addition, multiplication, …, the function whose value at m is the (m + 1)st prime, …</br></br></br></br></br></br>
				On the one hand, it is clear that every primitive recursive function should be regarded as being effectively calculable. On the other hand, the class of primitive recursive functions cannot possibly comprehend all total calculable functions, because one can easily "diagonalize out" of the class. That is, by suitably indexing the "family tree" of the primitive recursive functions, one can make a list f 0, f 1, f 2, … of all the one-place primitive recursive functions. One can then consider the diagonal function d (x ) = fx (x ) + 1. Then d cannot be primitive recursive; it differs from each fx at x. Nonetheless, if one makes the list tidely, the function d is effectively calculable. The conclusion is the class of primitive recursive functions is an extensive but proper subset of the total calculable functions.</br></br></br></br></br></br>
				Next, one can say that a k -place function h is obtained from the k + 1-place function g by minimalization and one writes
h (x⃗ )=μy [g (x⃗,y )=0]
if for each x⃗, the value h (x⃗ ) either is the number y such that g (x⃗, y ) = 0 and g (x⃗, s ) is defined and is nonzero for every s < y, if such a number y exists, or else is undefined, if no such number y exists. The idea behind this μ-operator is the idea of searching for the least number y that is the solution to an equation, by testing successively y = 0, 1, …</br></br></br></br>
				One can obtain the general recursive functions by adding minimalization to the closure methods. That is, a partial function is general recursive if it can be built up from the initial zero, successor, and projection functions by use of composition, primitive recursion, and minimalization.</br></br></br></br></br></br>
				The class of general recursive functions is (as Turing proved) exactly the same as the class of Turing computable functions. And Church's thesis therefore has the equivalent formulation that the concept of a general recursive function is the correct formalization of the informal concept of effective calculability.</br></br></br>
				What if one tries to diagonalize out of the class of general recursive functions, as one did for the primitive recursive functions? As will be argued later, one can again make a tidy list φ0, φ1, φ2, … of all the one-place general recursive partial functions. And one can define the diagonal function d (x ) = φx (x ) + 1. In this equation, d (x ) is undefined unless φx (x ) is defined. The diagonal function d is indeed among the general recursive partial functions, and hence is φk for some k, but d (k ) must be undefined. No contradiction results.</br></br></br></br></br></br>
				The class of primitive recursive functions was defined by Gödel, in his 1931 paper on the incompleteness theorems. Of course, the idea of defining functions on ℕ by recursion is much older and reflects the idea that the natural numbers are built up from the number 0 by repeated application of the successor function. The theory of the general recursive functions was worked out primarily by Stephen Cole Kleene, a student of Church.</br></br></br>
				The use of the word recursive in the context of the primitive recursive functions is entirely reasonable. Gödel, writing in German, had used simply rekursiv for the primitive recursive functions. Retaining the word recursive for the general recursive functions was a, however, historical accident. The class of general recursive functions—as this section shows—has several characterizations in which recursion (i.e., defining a function in terms of its other values, or using routines that call themselves) plays no role at all.</br></br></br>
				Nonetheless, the terminology became standard. What are here called the computable partial functions were until the late 1990s standardly called the partial recursive functions. And for that matter, computability theory was called recursive function theory for many years, and then recursion theory. And relations on ℕ were said to be recursive if their characteristic functions were general recursive functions.</br></br></br></br></br></br>
				An effort is now being made, however, to change what had been the standard terminology. Accordingly, this entry speaks of computable partial functions. And it will call a relation computable if its characteristic function is a computable function. Thus, the concept of a computable relation corresponds to the informal notion of a decidable relation. In any case, there is definitely a need to have separate adjectives for the informal concept (here, calculable is used for functions, and decidable for relations) and the formally defined concept (here, computable ).</br></br></br></br></br></br>
				</p>
			</div>
		</section>
	</div>
</body>
</html>